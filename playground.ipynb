{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "SolDablAnxTw",
        "outputId": "007a237d-d8e9-44aa-d21e-3fef73f9d647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 24 09:58:22 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "lIYdn1woOS1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e82e2234-3381-4d56-a330-fa4bd473cf47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git pull: Updating c19c31e..d7d7afc\n",
            "Fast-forward\n",
            " nlp_cw/trainer.py | 5 +++--\n",
            " 1 file changed, 3 insertions(+), 2 deletions(-)\n",
            "\n",
            "PIP install: 0\n"
          ]
        }
      ],
      "source": [
        "#@title Setup environment { display-mode: 'form' }\n",
        "# %%capture\n",
        "import os, subprocess, sys\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "if 'tensorboard' in ipython.extension_manager.loaded:\n",
        "  %reload_ext tensorboard\n",
        "else:\n",
        "  %load_ext tensorboard\n",
        "\n",
        "if 'autoreload' in ipython.extension_manager.loaded:\n",
        "  %reload_ext autoreload\n",
        "else:\n",
        "  %load_ext autoreload\n",
        "\n",
        "%autoreload 2\n",
        "\n",
        "src_dir = os.path.abspath(\"/content/COMM061-NLP-CW\")\n",
        "first_install = False\n",
        "if not os.path.exists(src_dir):\n",
        "  output = subprocess.run([\"git\", \"clone\", \"https://github.com/Cogniveon/COMM061-NLP-CW.git\"], capture_output=True)\n",
        "  print(\"Git clone:\", output.stderr.decode('utf-8'))\n",
        "  assert output.returncode == 0\n",
        "  first_install = True\n",
        "\n",
        "output = subprocess.run(\n",
        "    [\"git\", \"pull\", \"origin\", \"main\"],\n",
        "    cwd=src_dir,\n",
        "    capture_output=True,\n",
        "    # check=True,\n",
        ")\n",
        "print(\"Git pull:\", output.stdout.decode('utf-8'))\n",
        "assert output.returncode == 0\n",
        "\n",
        "\n",
        "already_up_to_date = 'Already up to date.' in output.stdout.decode('utf-8')\n",
        "\n",
        "if first_install or not already_up_to_date:\n",
        "  output = subprocess.run(\n",
        "      [\"pip\", \"install\", \"-e\", src_dir],\n",
        "      capture_output=True,\n",
        "      # check=True,\n",
        "  )\n",
        "  print(\"PIP install:\", output.returncode)\n",
        "  assert output.returncode == 0\n",
        "else:\n",
        "  print(\"Skipping install...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env WANDB_PROJECT COMM061-NLP-CW\n",
        "%env WANDB_LOG_MODEL end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pdeCHXGmOm8",
        "outputId": "92b6420f-57df-4160-b782-715c13b166aa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: WANDB_PROJECT=COMM061-NLP-CW\n",
            "env: WANDB_LOG_MODEL=end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nlp_cw import get_dataset, tokenize_dataset, init_trainer\n",
        "\n",
        "dataset, id2label, label2id, label_list, num_labels = get_dataset(\n",
        "    \"surrey-nlp/PLOD-CW\", [\"B-O\", \"B-AC\", \"I-AC\", \"B-LF\", \"I-LF\"]\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "22FC56Yp5bZW",
        "outputId": "bf16d567-d774-4852-85a9-ef4b64a1fa4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tokens', 'ner_tags'],\n",
              "        num_rows: 1072\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['tokens', 'ner_tags'],\n",
              "        num_rows: 126\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tokens', 'ner_tags'],\n",
              "        num_rows: 153\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"romainlhardy/roberta-large-finetuned-ner\"\n",
        "model_name = \"google-bert/bert-base-uncased\"\n",
        "\n",
        "tokenizer, tokenized_dataset = tokenize_dataset(dataset, model_name)"
      ],
      "metadata": {
        "id": "oyW3Hh2OI8Tw"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = init_trainer(\n",
        "    tokenizer,\n",
        "    label_list,\n",
        "    num_labels,\n",
        "    id2label,\n",
        "    label2id,\n",
        "    batch_size=16,\n",
        "    num_epochs=10,\n",
        "    model_name_or_path=model_name,\n",
        "    dataset=tokenized_dataset,\n",
        "    push_to_hub=True,\n",
        "    report_to=\"wandb\",\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "FmZ3nO6tP7s1",
        "outputId": "b9fd2ec5-3ca4-4769-f3b4-62e7d5bc3572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='536' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [536/670 05:59 < 01:30, 1.48 it/s, Epoch 8/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.632500</td>\n",
              "      <td>0.262892</td>\n",
              "      <td>0.903591</td>\n",
              "      <td>0.908956</td>\n",
              "      <td>0.906265</td>\n",
              "      <td>0.904260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.316900</td>\n",
              "      <td>0.229745</td>\n",
              "      <td>0.930936</td>\n",
              "      <td>0.913739</td>\n",
              "      <td>0.922257</td>\n",
              "      <td>0.918155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.199400</td>\n",
              "      <td>0.228232</td>\n",
              "      <td>0.931017</td>\n",
              "      <td>0.919347</td>\n",
              "      <td>0.925145</td>\n",
              "      <td>0.922278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.219305</td>\n",
              "      <td>0.936616</td>\n",
              "      <td>0.928583</td>\n",
              "      <td>0.932582</td>\n",
              "      <td>0.927775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.145700</td>\n",
              "      <td>0.234993</td>\n",
              "      <td>0.939494</td>\n",
              "      <td>0.937325</td>\n",
              "      <td>0.938408</td>\n",
              "      <td>0.933120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.108600</td>\n",
              "      <td>0.243504</td>\n",
              "      <td>0.941793</td>\n",
              "      <td>0.934026</td>\n",
              "      <td>0.937893</td>\n",
              "      <td>0.933120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.090800</td>\n",
              "      <td>0.253742</td>\n",
              "      <td>0.935661</td>\n",
              "      <td>0.928253</td>\n",
              "      <td>0.931942</td>\n",
              "      <td>0.927012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.079100</td>\n",
              "      <td>0.267502</td>\n",
              "      <td>0.939032</td>\n",
              "      <td>0.934851</td>\n",
              "      <td>0.936937</td>\n",
              "      <td>0.931745</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=536, training_loss=0.20418327646469003, metrics={'train_runtime': 332.6821, 'train_samples_per_second': 32.223, 'train_steps_per_second': 2.014, 'total_flos': 551939044151040.0, 'train_loss': 0.20418327646469003, 'epoch': 8.0})"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "ZUHU55QdP-yJ",
        "outputId": "4fb7ce06-0547-4803-df56-4489926e7645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.24625098705291748,\n",
              " 'eval_precision': 0.9399469143994691,\n",
              " 'eval_recall': 0.9345208642586178,\n",
              " 'eval_f1': 0.9372260358944668,\n",
              " 'eval_accuracy': 0.9323560848984578,\n",
              " 'eval_runtime': 1.2783,\n",
              " 'eval_samples_per_second': 98.57,\n",
              " 'eval_steps_per_second': 6.258,\n",
              " 'epoch': 9.0}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Commit to HuggingFace { display-mode: 'form' }\n",
        "commit_msg = 'google-bert-base-uncased-f1_937' #@param {type: \"string\"}\n",
        "trainer.push_to_hub(commit_msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "vgLXvLazlAhV",
        "outputId": "03692dcf-6ea9-4871-80c2-08b27ddd0221"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/cogniveon/nlpcw_bert-base-uncased-abbr/commit/f5e79b62885542d798456c67c92678f91236c6d5', commit_message='google-bert-base-uncased-f1_937', commit_description='', oid='f5e79b62885542d798456c67c92678f91236c6d5', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}